{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifieur VGG16 par fine tuning\n",
    "\n",
    "source : https://thebinarynotes.com/transfer-learning-keras-vgg16/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection, preprocessing\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation,GlobalMaxPooling2D\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.applications import VGG16\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "image_width = 224\n",
    "image_height = 224\n",
    "channels = 3\n",
    "batch_size = 64\n",
    " \n",
    "# Data directories\n",
    "train_dir = \"data/train\"\n",
    "test_dir = \"data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing dataframes\n",
    "data = pd.read_csv(\"data/data.csv\")\n",
    "\n",
    "data = data[[\"image\", \"category\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55b85ea15a1536d46b7190ad6fff8ce7.jpg</td>\n",
       "      <td>Home Furnishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b72c92c2f6c40268628ec5f14c6d590.jpg</td>\n",
       "      <td>Baby Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64d5d4a258243731dc7bbb1eef49ad74.jpg</td>\n",
       "      <td>Baby Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d4684dcdc759dd9cdf41504698d737d8.jpg</td>\n",
       "      <td>Home Furnishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6325b6870c54cd47be6ebfbffa620ec7.jpg</td>\n",
       "      <td>Home Furnishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>958f54f4c46b53c8a0a9b8167d9140bc.jpg</td>\n",
       "      <td>Baby Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>fd6cbcc22efb6b761bd564c28928483c.jpg</td>\n",
       "      <td>Baby Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>5912e037d12774bb73a2048f35a00009.jpg</td>\n",
       "      <td>Baby Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>c3edc504d1b4f0ba6224fa53a43a7ad6.jpg</td>\n",
       "      <td>Baby Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>f2f027ad6a6df617c9f125173da71e44.jpg</td>\n",
       "      <td>Baby Care</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     image         category\n",
       "0     55b85ea15a1536d46b7190ad6fff8ce7.jpg  Home Furnishing\n",
       "1     7b72c92c2f6c40268628ec5f14c6d590.jpg        Baby Care\n",
       "2     64d5d4a258243731dc7bbb1eef49ad74.jpg        Baby Care\n",
       "3     d4684dcdc759dd9cdf41504698d737d8.jpg  Home Furnishing\n",
       "4     6325b6870c54cd47be6ebfbffa620ec7.jpg  Home Furnishing\n",
       "...                                    ...              ...\n",
       "1045  958f54f4c46b53c8a0a9b8167d9140bc.jpg        Baby Care\n",
       "1046  fd6cbcc22efb6b761bd564c28928483c.jpg        Baby Care\n",
       "1047  5912e037d12774bb73a2048f35a00009.jpg        Baby Care\n",
       "1048  c3edc504d1b4f0ba6224fa53a43a7ad6.jpg        Baby Care\n",
       "1049  f2f027ad6a6df617c9f125173da71e44.jpg        Baby Care\n",
       "\n",
       "[1050 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting images into test and train directory\n",
    "\n",
    "import shutil\n",
    "\n",
    "train_data = pd.DataFrame()\n",
    "test_data = pd.DataFrame()\n",
    "\n",
    "sample_data = pd.read_csv(\"data/sample_data.csv\")\n",
    "for ind in data.index:\n",
    "    if data.loc[ind, \"image\"] in sample_data[\"image\"].values:\n",
    "        shutil.copy(\"data/Images/\"+data.loc[ind, \"image\"], \"data/test/\")\n",
    "        test_data = test_data.append(data.loc[ind])\n",
    "    else :\n",
    "        shutil.copy(\"data/Images/\"+data.loc[ind, \"image\"], \"data/train/\")\n",
    "        train_data = train_data.append(data.loc[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Baby Care                     15\n",
       "Beauty and Personal Care      15\n",
       "Computers                     15\n",
       "Home Decor & Festive Needs    15\n",
       "Home Furnishing               15\n",
       "Kitchen & Dining              15\n",
       "Watches                       15\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.groupby(by=\"category\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train and validation data\n",
    "train_df, validation_df = model_selection.train_test_split(train_data, test_size=0.1)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "validation_df = validation_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 850 validated image filenames belonging to 7 classes.\n",
      "Found 95 validated image filenames belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Creating train generator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")\n",
    " \n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df, \n",
    "    train_dir, \n",
    "    x_col='image',\n",
    "    y_col='category',\n",
    "    class_mode='categorical',\n",
    "    target_size=(image_height, image_width),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Creating Validation Generator\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    validation_df, \n",
    "    train_dir, \n",
    "    x_col='image',\n",
    "    y_col='category',\n",
    "    class_mode='categorical',\n",
    "    target_size=(image_height, image_width),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 3591      \n",
      "=================================================================\n",
      "Total params: 14,980,935\n",
      "Trainable params: 7,345,671\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Loading Model\n",
    "pretrained_model = VGG16(input_shape=(image_height, image_width, channels), include_top=False, weights=\"imagenet\")\n",
    "pretrained_model.summary()\n",
    " \n",
    "# Freezing the layers\n",
    "for layer in pretrained_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "for layer in pretrained_model.layers[15:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Modification of pretrained model\n",
    "last_layer = pretrained_model.get_layer('block5_pool')\n",
    "last_output = last_layer.output\n",
    " \n",
    "x = GlobalMaxPooling2D()(last_output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = layers.Dense(7, activation='sigmoid')(x)\n",
    " \n",
    "# Creating a new model\n",
    "model = Model(pretrained_model.input, x)\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-4d06d89bac3c>:5: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/20\n",
      "10/13 [======================>.......] - ETA: 25s - loss: 2.0881 - accuracy: 0.1484"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samuel/.local/lib/python3.8/site-packages/PIL/Image.py:2834: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 121s 9s/step - loss: 2.0920 - accuracy: 0.1526 - val_loss: 2.0074 - val_accuracy: 0.1406\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 113s 9s/step - loss: 2.0399 - accuracy: 0.1412 - val_loss: 1.9478 - val_accuracy: 0.1562\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 114s 9s/step - loss: 1.9566 - accuracy: 0.1718 - val_loss: 1.8965 - val_accuracy: 0.1875\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 119s 9s/step - loss: 1.9412 - accuracy: 0.1755 - val_loss: 1.9084 - val_accuracy: 0.1562\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 113s 9s/step - loss: 1.9273 - accuracy: 0.1870 - val_loss: 1.8902 - val_accuracy: 0.2031\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 112s 9s/step - loss: 1.8909 - accuracy: 0.2061 - val_loss: 1.8679 - val_accuracy: 0.2344\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 113s 9s/step - loss: 1.9060 - accuracy: 0.1972 - val_loss: 1.8572 - val_accuracy: 0.2188\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 119s 9s/step - loss: 1.8860 - accuracy: 0.2392 - val_loss: 1.8727 - val_accuracy: 0.1719\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 111s 9s/step - loss: 1.8879 - accuracy: 0.2048 - val_loss: 1.8271 - val_accuracy: 0.2500\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 114s 9s/step - loss: 1.8741 - accuracy: 0.2328 - val_loss: 1.8369 - val_accuracy: 0.3125\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 113s 9s/step - loss: 1.8587 - accuracy: 0.2583 - val_loss: 1.8229 - val_accuracy: 0.1875\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 113s 9s/step - loss: 1.8476 - accuracy: 0.2481 - val_loss: 1.7984 - val_accuracy: 0.2656\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 119s 9s/step - loss: 1.8323 - accuracy: 0.2788 - val_loss: 1.7757 - val_accuracy: 0.3594\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 113s 9s/step - loss: 1.8027 - accuracy: 0.3104 - val_loss: 1.7750 - val_accuracy: 0.3438\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 111s 9s/step - loss: 1.8153 - accuracy: 0.2659 - val_loss: 1.7696 - val_accuracy: 0.3438\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 111s 9s/step - loss: 1.7886 - accuracy: 0.3295 - val_loss: 1.7335 - val_accuracy: 0.4062\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 111s 9s/step - loss: 1.7710 - accuracy: 0.3193 - val_loss: 1.7165 - val_accuracy: 0.4375\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 114s 9s/step - loss: 1.7639 - accuracy: 0.3206 - val_loss: 1.7113 - val_accuracy: 0.4375\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 112s 9s/step - loss: 1.7239 - accuracy: 0.3511 - val_loss: 1.6509 - val_accuracy: 0.4531\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 112s 9s/step - loss: 1.6977 - accuracy: 0.3372 - val_loss: 1.6743 - val_accuracy: 0.4375\n"
     ]
    }
   ],
   "source": [
    "# Training model\n",
    "n_training_samples = len(train_df)\n",
    "n_validation_samples = len(validation_df)\n",
    " \n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=n_validation_samples//batch_size,\n",
    "    steps_per_epoch=n_training_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 105 validated image filenames.\n",
      "WARNING:tensorflow:From <ipython-input-11-e858fd9cc950>:16: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n"
     ]
    }
   ],
   "source": [
    "nb_samples = test_data.shape[0]\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "test_generator = test_gen.flow_from_dataframe(\n",
    "    test_data, \n",
    "    test_dir, \n",
    "    x_col='image',\n",
    "    y_col=None,\n",
    "    class_mode=None,\n",
    "    batch_size=batch_size,\n",
    "    target_size=(image_height, image_width),\n",
    "    shuffle=False\n",
    ")\n",
    " \n",
    "# Testing Model\n",
    "predict = model.predict_generator(test_generator, steps=np.ceil(nb_samples/batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_indices=np.argmax(predict,axis=1)\n",
    "labels = (train_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12380952380952381"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance of the model\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "metrics.accuracy_score(predictions, sample_data[\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
