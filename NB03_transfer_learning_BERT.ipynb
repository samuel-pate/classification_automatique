{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifieur BERT par fine tuning\n",
    "source : https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/samuel/.local/lib/python3.8/site-packages (3.0.2)\n",
      "Requirement already satisfied: numpy in /home/samuel/.local/lib/python3.8/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: packaging in /home/samuel/.local/lib/python3.8/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/samuel/.local/lib/python3.8/site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/samuel/.local/lib/python3.8/site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied: sacremoses in /home/samuel/.local/lib/python3.8/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: filelock in /home/samuel/.local/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /home/samuel/.local/lib/python3.8/site-packages (from transformers) (0.1.94)\n",
      "Requirement already satisfied: requests in /home/samuel/.local/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in /home/samuel/.local/lib/python3.8/site-packages (from transformers) (0.8.1rc1)\n",
      "Requirement already satisfied: six in /home/samuel/.local/lib/python3.8/site-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/samuel/.local/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: joblib in /home/samuel/.local/lib/python3.8/site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: click in /home/samuel/.local/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/lib/python3/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/samuel/.local/lib/python3.8/site-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/samuel/.local/lib/python3.8/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/samuel/.local/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "# specify CPU\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "\n",
    "df = pd.read_csv(\"data/data.csv\")\n",
    "\n",
    "encode_dict = dict()\n",
    "def encode_cat(x):\n",
    "    if x not in encode_dict.keys():\n",
    "        encode_dict[x]=len(encode_dict)\n",
    "    return encode_dict[x]\n",
    "\n",
    "df['label'] = df['category'].apply(lambda x: encode_cat(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>product_name</th>\n",
       "      <th>image</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Elegance Polyester Multicolor Abstract Eyelet ...</td>\n",
       "      <td>55b85ea15a1536d46b7190ad6fff8ce7.jpg</td>\n",
       "      <td>Elegance Polyester Multicolor Abstract Eyelet ...</td>\n",
       "      <td>Home Furnishing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sathiyas Cotton Bath Towel</td>\n",
       "      <td>7b72c92c2f6c40268628ec5f14c6d590.jpg</td>\n",
       "      <td>Sathiyas Cotton Bath Towel\\nSpecifications of ...</td>\n",
       "      <td>Baby Care</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Eurospa Cotton Terry Face Towel Set</td>\n",
       "      <td>64d5d4a258243731dc7bbb1eef49ad74.jpg</td>\n",
       "      <td>Eurospa Cotton Terry Face Towel Set\\nKey Featu...</td>\n",
       "      <td>Baby Care</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>SANTOSH ROYAL FASHION Cotton Printed King size...</td>\n",
       "      <td>d4684dcdc759dd9cdf41504698d737d8.jpg</td>\n",
       "      <td>SANTOSH ROYAL FASHION Cotton Printed King size...</td>\n",
       "      <td>Home Furnishing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Jaipur Print Cotton Floral King sized Double B...</td>\n",
       "      <td>6325b6870c54cd47be6ebfbffa620ec7.jpg</td>\n",
       "      <td>Jaipur Print Cotton Floral King sized Double B...</td>\n",
       "      <td>Home Furnishing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                       product_name  \\\n",
       "0           0  Elegance Polyester Multicolor Abstract Eyelet ...   \n",
       "1           1                         Sathiyas Cotton Bath Towel   \n",
       "2           2                Eurospa Cotton Terry Face Towel Set   \n",
       "3           3  SANTOSH ROYAL FASHION Cotton Printed King size...   \n",
       "4           4  Jaipur Print Cotton Floral King sized Double B...   \n",
       "\n",
       "                                  image  \\\n",
       "0  55b85ea15a1536d46b7190ad6fff8ce7.jpg   \n",
       "1  7b72c92c2f6c40268628ec5f14c6d590.jpg   \n",
       "2  64d5d4a258243731dc7bbb1eef49ad74.jpg   \n",
       "3  d4684dcdc759dd9cdf41504698d737d8.jpg   \n",
       "4  6325b6870c54cd47be6ebfbffa620ec7.jpg   \n",
       "\n",
       "                                                text         category  label  \n",
       "0  Elegance Polyester Multicolor Abstract Eyelet ...  Home Furnishing      0  \n",
       "1  Sathiyas Cotton Bath Towel\\nSpecifications of ...        Baby Care      1  \n",
       "2  Eurospa Cotton Terry Face Towel Set\\nKey Featu...        Baby Care      1  \n",
       "3  SANTOSH ROYAL FASHION Cotton Printed King size...  Home Furnishing      0  \n",
       "4  Jaipur Print Cotton Floral King sized Double B...  Home Furnishing      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train dataset into train, validation and test sets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['label'])\n",
    "\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storage/OpenClassRooms/Projet 6/transformers/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1938: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 300\n",
    "\n",
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = MAX_LENGTH,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = MAX_LENGTH,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = MAX_LENGTH,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert lists to tensors\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 64\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        self.bert = bert \n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "\n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,7)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "\n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to CPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5)          # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storage/OpenClassRooms/Projet 6/transformers/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass classes=[0 1 2 3 4 5 6], y=994    4\n",
      "99     5\n",
      "367    6\n",
      "376    5\n",
      "149    4\n",
      "      ..\n",
      "81     3\n",
      "348    6\n",
      "501    2\n",
      "220    1\n",
      "479    6\n",
      "Name: label, Length: 735, dtype: int64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
    "\n",
    "print(\"Class Weights:\",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# push to CPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to cpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "    print(\"\\nEvaluating...\")\n",
    "\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to cpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss and accuracy between actual and predicted values\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            y_preds = np.argmax(preds, axis = 1)\n",
    "            accuracy = accuracy_score(y_preds, labels)\n",
    "            \n",
    "            total_loss = total_loss + loss.item()\n",
    "            total_accuracy = total_accuracy + accuracy\n",
    "            \n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "            \n",
    "            \n",
    "    # compute the validation loss and accuracy of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "    avg_accuracy = total_accuracy / len(val_dataloader)\n",
    "    \n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, avg_accuracy, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 20\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.951\n",
      "Validation Loss: 1.946\n",
      "Validation Accuracy: 0.133\n",
      "\n",
      " Epoch 2 / 20\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.942\n",
      "Validation Loss: 1.943\n",
      "Validation Accuracy: 0.133\n",
      "\n",
      " Epoch 3 / 20\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.943\n",
      "Validation Loss: 1.939\n",
      "Validation Accuracy: 0.150\n",
      "\n",
      " Epoch 4 / 20\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.936\n",
      "Validation Loss: 1.936\n",
      "Validation Accuracy: 0.162\n",
      "\n",
      " Epoch 5 / 20\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.932\n",
      "Validation Loss: 1.934\n",
      "Validation Accuracy: 0.189\n",
      "\n",
      " Epoch 6 / 20\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.929\n",
      "Validation Loss: 1.932\n",
      "Validation Accuracy: 0.233\n",
      "\n",
      " Epoch 7 / 20\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.925\n",
      "Validation Loss: 1.931\n",
      "Validation Accuracy: 0.215\n",
      "\n",
      " Epoch 8 / 20\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.921\n",
      "Validation Loss: 1.929\n",
      "Validation Accuracy: 0.165\n",
      "\n",
      " Epoch 9 / 20\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.920\n",
      "Validation Loss: 1.926\n",
      "Validation Accuracy: 0.187\n",
      "\n",
      " Epoch 10 / 20\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.919\n",
      "Validation Loss: 1.924\n",
      "Validation Accuracy: 0.177\n",
      "\n",
      " Epoch 11 / 20\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.914\n",
      "Validation Loss: 1.922\n",
      "Validation Accuracy: 0.254\n",
      "\n",
      " Epoch 12 / 20\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.909\n",
      "Validation Loss: 1.920\n",
      "Validation Accuracy: 0.292\n",
      "\n",
      " Epoch 13 / 20\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.907\n",
      "Validation Loss: 1.919\n",
      "Validation Accuracy: 0.243\n",
      "\n",
      " Epoch 14 / 20\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.911\n",
      "Validation Loss: 1.917\n",
      "Validation Accuracy: 0.230\n",
      "\n",
      " Epoch 15 / 20\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.902\n",
      "Validation Loss: 1.915\n",
      "Validation Accuracy: 0.237\n",
      "\n",
      " Epoch 16 / 20\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.903\n",
      "Validation Loss: 1.914\n",
      "Validation Accuracy: 0.247\n",
      "\n",
      " Epoch 17 / 20\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.904\n",
      "Validation Loss: 1.912\n",
      "Validation Accuracy: 0.258\n",
      "\n",
      " Epoch 18 / 20\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.900\n",
      "Validation Loss: 1.909\n",
      "Validation Accuracy: 0.317\n",
      "\n",
      " Epoch 19 / 20\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.895\n",
      "Validation Loss: 1.908\n",
      "Validation Accuracy: 0.376\n",
      "\n",
      " Epoch 20 / 20\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.895\n",
      "Validation Loss: 1.907\n",
      "Validation Accuracy: 0.300\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, valid_accuracy, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')\n",
    "    print(f'Validation Accuracy: {valid_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.64      0.43        22\n",
      "           1       0.50      0.26      0.34        23\n",
      "           2       0.44      0.30      0.36        23\n",
      "           3       0.29      0.32      0.30        22\n",
      "           4       0.25      0.61      0.36        23\n",
      "           5       1.00      0.05      0.09        22\n",
      "           6       0.57      0.17      0.27        23\n",
      "\n",
      "    accuracy                           0.34       158\n",
      "   macro avg       0.48      0.34      0.31       158\n",
      "weighted avg       0.48      0.34      0.31       158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# performance of the model\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
